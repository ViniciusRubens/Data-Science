# -*- coding: utf-8 -*-
"""SERIES-TEMPORAIS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IjN1fYF2V99BSK4PAY8jLtu0B_YeS99v

# <center>Séries temporais</center>
___

Um dos grandes desafios das empresas é a previsão de vendas. Um bom modelo de previsão das vendas da empresa permite um melhor planejamento de gastos das empresas e da produção, permite uma estimação de lucros e até mesmo auxilia a empresa a determinar metas, avaliar o seu desempenho e ter uma melhor visão de futuro, ajudando na atração de possíveis investidores.

Esse notebook contém um estudo prático sobre esse tema. O *dataset* que utilizaremos vem originalmente de um desafio de recrutamento do Walmart - que pode ser acessado [aqui](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data). Ele contém vendas anônimas por departamento para 45 lojas Walmart, bem como variáveis de apoio. Cada loja contém um número de departamentos, além disso, a Walmart realiza vários eventos promocionais de descontos (ou `Markdown`) ao longo do ano.

Assim, o objetivo proposto aqui é de **prever as vendas semanais de cada departamento de cada loja da Walmart**.


Vamos começar lendo o nosso *dataset*
"""

# Importando as bibliotecas principais utilizadas
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt

# Importando o dataset
walmart = pd.read_csv("walmart_sales.csv")

# Mostrando o formato e as primeiras linhas do dataset
print(walmart.shape)
walmart.head()

"""Já vimos que existem alguns valores nulos. Vamos utilizar os métodos `info` e `describe` para identificar com maior detalhe algumas informações necessárias para fazer uma limpeza básica nos nossos dados."""

walmart.info()

walmart.describe(include='all')

"""Por meio de uma verificação visual, já sabemos que temos valores nulos, mas vamos confirmar isso através delinhas de código:"""

walmart.isna().any()

"""Dessa rápida análise, conseguimos tirar alguns insights para o tratamento dos nossos dados:

1. A coluna 'Date' precisa ser transformada para o formato 'DateTime' para trabalharmos melhor com ela
2. A coluna 'IsHoliday' é booleana e precisa ser transformada em numérica
3. A coluna 'Type' representa dados categóricos em formato de string. Para o input no modelo, essa coluna precisa ser transformada em Dummy
4. As colunas 'MarkDown' possuem uma significativa quantidade de dados vazios
5. As colunas 'Store' e 'Dept' são numéricas e discretas

Vamos iniciar transformando a variável `'Date'` no formato `'DateTime'` para facilitar a manipulação do *dataset*
"""

# Transforme a variavel 'Date' em DateTime
walmart['Date'] =  pd.to_datetime(walmart['Date'])

# Ordenando os valores do dataset inicialmente por 'Date', depois 'Store' e 'Dept'
walmart.sort_values(["Date","Store","Dept"], inplace=True)

# walmart_types = walmart.dtypes

print(walmart[['Date']].info())
walmart.head(10)

"""Podemos ver que existem várias linhas com datas iguais. Isso significa que, para uma mesma data, existem várias linhas com dados diferentes, o que não é ideal quando queremos trabalhar com séries temporais, já que queremos que o tempo seja o índice do nosso *dataset*. Contudo, se você perceber do resultado do `.head` acima, os dados das colunas 'Store' e 'Dept' são diferentes para cada data. Vamos analisar se, para uma mesma data, loja e departamento, existem informações duplicadas."""

# Verificando o número de datas distintas
len(walmart.Date.unique())

# Verificando se uma mesma data possui dados com store e dept iguais
walmart.duplicated(['Date','Store','Dept']).sum()

"""Perfeito! Notamos então que, no mesmo *dataset*, temos diferentes séries temporais, cada uma caracterizada por essas três colunas: 'Date', 'Store' e 'Dept'. Vamos visualizar a evolução das vendas ao longo do tempo para uma dada loja e departamento.

Obs.: A barra abaixo auxilia a explorar diferentes séries temporais.
"""

# Plotando a série temporal para uma dada loja e departamento

from IPython.display import display
from ipywidgets import interact, IntSlider


def plot_serie(store, dept):
    plot_filter = walmart[(walmart['Store']==store) & (walmart['Dept']==dept)]

    fig, ax = plt.subplots(figsize=(10,3))
    ax.plot(plot_filter['Date'],plot_filter['Weekly_Sales'])

    plt.show()

# Criando o Slider interativo
_ = interact(plot_serie,
             store=IntSlider(min=walmart.Store.min(), max=walmart.Store.max(), step = 1, value = 9),
             dept =IntSlider(min=walmart.Dept.min(), max=walmart.Dept.max(), step = 1, value = 71)
             )

"""Agora vamos tratar os dados e criar uma feature.

Como vimos, os valores nulos são das colunas Markdown, que representam os descontos dados pelos departamentos, dessa forma, vamos substituir os valores nulos do dataset por zeros.
"""

walmart = walmart.fillna(0)

"""Em seguida, vamos transformar a variável 'IsHoliday'em int."""

walmart['IsHoliday'] = pd.to_numeric(walmart['IsHoliday']).astype('int')

"""Criaremos a feature 'days_from' que indica o intervalo de dias desde a data mínima (int)."""

min_date = walmart['Date'].min()                             # Encontre a data mínima da coluna de datas
walmart['days_from'] = walmart['Date'] - min_date         # Calcule a diferença entre a data e a data mínima (formato timedelta)
walmart.days_from = walmart.days_from.dt.days      # Pegando apenas os "dias" como int

"""Agora criaremos Dummies para as variáveis categóricas do dataset e verificaremos as novas informações.

A função get_dummies no Pandas é usada para criar variáveis dummy (também conhecidas como variáveis fictícias ou variáveis indicadoras) a partir de uma coluna que contenha dados categóricos. Essa função é frequentemente usada em análise de dados e modelagem estatística, especialmente em tarefas de aprendizado de máquina.

Quando você tem uma coluna com valores categóricos, como "gato", "cachorro" e "peixe", o get_dummies cria colunas separadas para cada categoria e preenche essas colunas com 0s e 1s para indicar a presença ou ausência de cada categoria em cada observação de dados.
"""

walmart = pd.get_dummies(walmart)
walmart.info()

"""Agora vamos realizar a separação do nosso *dataset* em conjunto de treino e teste"""

from sklearn.model_selection import train_test_split # Importa a função usada para a separação de treino e teste
from sklearn.model_selection import TimeSeriesSplit # Importa a função usada para a separação da validação cruzada em séries temporais

# Separando nossa variável de interesse.
X = walmart.drop(['Date','Weekly_Sales'], axis=1)
y = walmart['Weekly_Sales']

# Utilize a função para dividir os dados em treino e teste
X_training, X_test, y_training, y_test = train_test_split(X, y,
                                                        test_size=0.25,
                                                        shuffle=False)

print("Conjunto de treino X:", X_training.shape)
print("Conjunto de treino y:", y_training.shape)
print("Conjunto de teste  X:", X_test.shape)
print("Conjunto de teste  y:", y_test.shape)

# utilizando uma janela expansiva com n_splits=10 para a validação cruzada
cv = TimeSeriesSplit(n_splits=10)

"""## Modelagem

Agora iremos partir para a segunda etapa do nosso projeto, a modelagem.

#### Random Forest

Vamos iniciar treinando um modelo de *random forest* com validação cruzada e *grid search*.

O *grid search* deve variar, pelo menos, os parâmetros `n_estimators` e `max_depth`, mas você pode incluir outros.

Após o treino, criaremos o *dataframe* `cv_results` com os resultados de cada iteração e o dicionário `cv_best_params` com os valores da melhor combinação de parâmetros.
"""

# Random Forest com Validação cruzada e Grid Search para Séries temporais

from sklearn.metrics import (explained_variance_score,
                             mean_absolute_error,
                             mean_squared_error,
                             mean_squared_log_error,
                             r2_score)
from sklearn.ensemble import RandomForestRegressor # Modelo para aplicar de Random Forest
from sklearn.model_selection import GridSearchCV  # Modelo para aplicar Grid Search e Cross-Validation

# Defina os valores possíveis para os parâmetros a serem testados
params = {'n_estimators': [100],
          'max_depth': [10]}

#params = {'n_estimators': [50, 100],    testados
#          'max_depth': [5, 10]}

# Crie o modelo de random forest
rf_model_cv_gs = RandomForestRegressor()

# Crie o objeto para grid search
grid_search = GridSearchCV(rf_model_cv_gs,
                           param_grid=params,
                           return_train_score=True,
                           scoring='r2',
                           cv=cv)

# Treine o modelo com Grid Search
grid_search.fit(X_training, y_training)

# Estruture em um DataFrame os resultados da validação cruzada (cv_results)
cv_results = pd.DataFrame(grid_search.cv_results_)

# Armazene a melhor combinação de hiperparâmetros
cv_best_params = grid_search.best_params_
print('\n Best hyperparameters:')
print(cv_best_params)

"""Agora vamos treinar o modelo final de *random forest* com os melhores parâmetros obtidos no *grid search* do exercício anterior."""

# Imprimindo o score médio nos conjuntos de treino
print("Pontuação média nos dados de treino: {:.3f} +/- {:.3f}".format(cv_results[cv_results.rank_test_score == 1].mean_train_score.values[0],
                                                                     cv_results[cv_results.rank_test_score == 1].std_train_score.values[0]))
# Imprimindo o score médio nos conjuntos de validação
print("Pontuação média nos dados de validação: {:.3f} +/- {:.3f}".format(cv_results[cv_results.rank_test_score == 1].mean_test_score.values[0],
                                                                     cv_results[cv_results.rank_test_score == 1].std_test_score.values[0]))

# Configure o modelo com a melhor combinação de hiperparâmetros
rf_model_cv_gs.set_params(n_estimators = cv_best_params['n_estimators'],
                        max_depth = cv_best_params['max_depth'])

# Treine um modelo com a melhor combinação de hiperparâmetros
rf_model_cv_gs.fit(X_training, y_training)
best_model_params = rf_model_cv_gs.get_params()

"""Por fim, vamos calcular o RMSE do modelo de *random forest* final na base de teste."""

y_test_pred_rf = rf_model_cv_gs.predict(X_test)
rmse_test_rf = math.sqrt(mean_squared_error(y_test, y_test_pred_rf))
print("RMSE do modelo no conjunto de teste:", rmse_test_rf)

"""Vamos desenhar gráficos para comparar as previsões que com os dados reais.

Obs.: Novamente, basta deslizar a barra para explorar diferentes séries temporais.
"""

# Plotando a série temporal dos dados previstos para uma dada loja e departamento

from IPython.display import display
from ipywidgets import interact, IntSlider

# juntando a previsão e os dados originais no mesmo dataset para a plotagem
training = pd.concat([X_training[['Store','Dept']], y_training], axis=1)
training['Predicted'] = rf_model_cv_gs.predict(X_training)
training = training.join(walmart['Date'])

test = pd.concat([X_test[['Store','Dept']], y_test], axis=1)
test['Predicted'] = rf_model_cv_gs.predict(X_test)
test = test.join(walmart['Date'])

# função de plot interativo, dada uma loja e um departamento
def plot_prediction(store, dept):
    # Filtrando a loja e o departamento para o plot
    plot_train = training[(training['Store']==store) & (training['Dept']==dept)]
    plot_test  = test[(test['Store']==store) & (test['Dept']==dept)]

    # Plotando no tempo os dados de treino e teste originais e previstos
    fig, ax = plt.subplots(figsize=(10,3))
    ax.plot(plot_train['Date'], plot_train['Weekly_Sales'])
    ax.plot(plot_train['Date'], plot_train['Predicted'], color='yellow')
    ax.plot(plot_test['Date'], plot_test['Weekly_Sales'], color='blue')
    ax.plot(plot_test['Date'], plot_test['Predicted'], color='orange')
    plt.show()

# Criando o Slider interativo
_ = interact(plot_prediction,
             store=IntSlider(min=walmart.Store.min(), max=walmart.Store.max(), step=1, value=20),
             dept =IntSlider(min=walmart.Dept.min(), max=walmart.Dept.max(), step=1, value=72)
             )